{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _connect_mongo(host, port, db):\n",
    "    \"\"\" A util for making a connection to mongo \"\"\"\n",
    "    try:\n",
    "        client = pymongo.MongoClient(host, port)\n",
    "        client.server_info()\n",
    "    except pymongo.errors.ServerSelectionTimeoutError as err:\n",
    "        print(err)\n",
    "        print(\"Are you sure your database is on and this can reach it?\") \n",
    "        raise ConnectionError\n",
    "    return client[db]\n",
    "\n",
    "\n",
    "def read_mongo(db, collection, query={}, host='localhost', port=27017, no_id=True):\n",
    "    \"\"\" Read from Mongo and Store into DataFrame \"\"\"\n",
    "    # Connect to MongoDB\n",
    "    db = _connect_mongo(host=host, port=port, db=db)\n",
    "\n",
    "    # Make a query to the specific DB and Collection\n",
    "    cursor = db[collection].find(query)\n",
    "    # Expand the cursor and construct the DataFrame\n",
    "    df =  pd.DataFrame(list(cursor))\n",
    "    # Delete the _id\n",
    "    if no_id:\n",
    "        del df['_id']\n",
    "\n",
    "    return df\n",
    "train_df = read_mongo(\"NETWORK\", \"train\")\n",
    "test_df = read_mongo(\"NETWORK\", \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head(5)\n",
    "print(len(train_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head(5)\n",
    "print(len(test_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset have the same amount of features. Of these, protocol type, service, flag and attack are categorical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df.isnull().values.any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check how the rest of the data looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a mix of ints, floats and strings. No null values that need to be cleand, however."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check string answer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_df[\"service\"].drop_duplicates().values))\n",
    "print(len(test_df[\"service\"].drop_duplicates().values))\n",
    "print(\"------------------------------------------------\")\n",
    "train_df[\"service\"].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_df[\"protocol_type\"].drop_duplicates().values))\n",
    "print(len(test_df[\"protocol_type\"].drop_duplicates().values))\n",
    "print(\"------------------------------------------------\")\n",
    "train_df[\"protocol_type\"].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_df[\"flag\"].drop_duplicates().values))\n",
    "print(len(test_df[\"flag\"].drop_duplicates().values))\n",
    "print(\"------------------------------------------------\")\n",
    "train_df[\"flag\"].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_df[\"attack\"].drop_duplicates().values))\n",
    "print(len(test_df[\"attack\"].drop_duplicates().values))\n",
    "print(\"------------------------------------------------\")\n",
    "train_df[\"attack\"].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_df[\"num_outbound_cmds\"].drop_duplicates())\n",
    "print(train_df[\"num_outbound_cmds\"].drop_duplicates())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our labeled data does not look the same. This means we have to clean up our data. We would have to do some of this anyways, as models can't handle categorical data. However, we are putting all attacks as malicous and aim to try to understand what is normal instead. Meaning, we are going to be performing one class anomaly detection. We are going to do one hot encoding on our data. We are also going to drop num outbound cmds, as it's all 0:s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.drop(\"num_outbound_cmds\", axis=1, inplace=True)\n",
    "test_df.drop(\"num_outbound_cmds\", axis=1, inplace=True)\n",
    "assert test_df.isnull().values.any() == False\n",
    "assert train_df.isnull().values.any() == False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encoder_mapping(dataframe: pd.DataFrame, coloumn: str):\n",
    "    labels = list(dataframe[coloumn].drop_duplicates().values)\n",
    "    labels.sort()\n",
    "\n",
    "    mapping = {}\n",
    "    for index, label in enumerate(labels):\n",
    "        mapping[label] = index+1\n",
    "\n",
    "    return mapping\n",
    "\n",
    "def transform_label(dataframe: pd.DataFrame, coloumns: list):\n",
    "    for coloumn in coloumns:\n",
    "        mapping = label_encoder_mapping(dataframe, coloumn)\n",
    "        dataframe[coloumn] = dataframe[coloumn].map(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_label(train_df, [\"flag\", \"protocol_type\", \"service\"])\n",
    "transform_label(test_df, [\"flag\", \"protocol_type\", \"service\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_attack(dataframe):\n",
    "    labels = list(dataframe[\"attack\"].drop_duplicates().values)\n",
    "    labels.sort()\n",
    "\n",
    "    mapping = {}\n",
    "    for index, label in enumerate(labels):\n",
    "        if label == \"normal\":\n",
    "            mapping[label] = 0\n",
    "        else:\n",
    "            mapping[label] = 1\n",
    "    dataframe[\"attack\"] = dataframe[\"attack\"].map(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transform_attack(train_df)\n",
    "transform_attack(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(5,10))\n",
    "distribution = train_df[\"attack\"].value_counts().reset_index()\n",
    "distribution.columns = [\"attack\", \"count\"]\n",
    "distribution.sort_values(by=\"attack\", ascending=True, inplace=True)\n",
    "print(distribution)\n",
    "\n",
    "palette_color = sns.color_palette('dark') \n",
    "axes[0].pie(data=distribution, labels=\"attack\", x=\"count\", colors=palette_color)\n",
    "axes[0].set_title(\"Train Attack Distribution\")\n",
    "\n",
    "distribution = test_df[\"attack\"].value_counts().reset_index()\n",
    "distribution.columns = [\"attack\", \"count\"]\n",
    "distribution.sort_values(by=\"attack\", ascending=True, inplace=True)\n",
    "print(distribution)\n",
    "\n",
    "palette_color = sns.color_palette('dark') \n",
    "axes[1].pie(data=distribution, labels=\"attack\", x=\"count\", colors=palette_color)\n",
    "axes[1].set_title(\"Test Attack Distribution\")\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalise values\n",
    "Between 0 and 1, except for attack. To avoid vanishing / exploding gradients, normalize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "without_attack = train_df.drop([\"attack\"], axis=1, inplace=False)\n",
    "normalized_train_df=(without_attack-without_attack.mean())/without_attack.std()\n",
    "normalized_train_df[\"attack\"] = train_df[\"attack\"]\n",
    "normalized_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "without_attack = test_df.drop([\"attack\"], axis=1, inplace=False)\n",
    "normalized_test_df=(without_attack-without_attack.mean())/without_attack.std()\n",
    "normalized_test_df[\"attack\"] = test_df[\"attack\"]\n",
    "normalized_test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://arxiv.org/pdf/1607.00148.pdf\n",
    "class LSTMAutoEncoder(nn.Module):\n",
    "    def __init__(self, num_layers, hidden_size, nb_feature, dropout=0, device=torch.device('cpu')):\n",
    "        super(LSTMAutoEncoder, self).__init__()\n",
    "        self.device = device\n",
    "        self.encoder = Encoder(num_layers, hidden_size, nb_feature, dropout, device)\n",
    "        self.decoder = Decoder(num_layers, hidden_size, nb_feature, dropout, device)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        output = torch.zeros(size=input_seq.shape, dtype=torch.float)\n",
    "        encoder_output = self.encoder(input_seq)  \n",
    "\n",
    "        input_decoder = encoder_output[:, -1, :].unsqueeze(1)  # shape: [batch, 1, hidden_size]\n",
    "        \n",
    "        decoder_hidden = (\n",
    "            torch.randn((self.decoder.num_layers, input_seq.size(0), self.decoder.hidden_size)).to(self.device),\n",
    "            torch.randn((self.decoder.num_layers, input_seq.size(0), self.decoder.hidden_size)).to(self.device)\n",
    "        )\n",
    "        \n",
    "        for i in range(input_seq.shape[1] - 1, -1, -1):\n",
    "            output_decoder, decoder_hidden = self.decoder(input_decoder, decoder_hidden)\n",
    "            input_decoder = output_decoder\n",
    "            output[:, i, :] = output_decoder[:, 0, :]\n",
    "        \n",
    "        return output\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_layers, hidden_size, nb_feature, dropout=0, device=torch.device('cpu')):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.input_size = nb_feature\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.device = device\n",
    "        self.lstm = nn.LSTM(input_size=nb_feature, hidden_size=hidden_size,\n",
    "                            num_layers=num_layers, batch_first=True, dropout=dropout, bias=True)\n",
    "        self.hidden_cell = None\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        self.hidden_cell = (\n",
    "            torch.randn((self.num_layers, batch_size, self.hidden_size), dtype=torch.float).to(self.device),\n",
    "            torch.randn((self.num_layers, batch_size, self.hidden_size), dtype=torch.float).to(self.device)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        self.initHidden(input_seq.shape[0])\n",
    "        output, self.hidden_cell = self.lstm(input_seq, self.hidden_cell)\n",
    "        return output \n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_layers, hidden_size, nb_feature, dropout=0, device=torch.device('cpu')):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.input_size = nb_feature\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.device = device\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=hidden_size, hidden_size=hidden_size,\n",
    "                            num_layers=num_layers, batch_first=True, dropout=dropout, bias=True)\n",
    "        self.linear = nn.Linear(in_features=hidden_size, out_features=nb_feature)\n",
    "\n",
    "    def forward(self, input_seq, hidden_cell):\n",
    "        output, hidden_cell = self.lstm(input_seq, hidden_cell)\n",
    "        output = self.linear(output)\n",
    "        return output, hidden_cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe: pd.DataFrame, device = None, transform=None, target_transform=None):\n",
    "        self.df_labels = dataframe[\"attack\"]\n",
    "        self.df = dataframe.drop([\"attack\"], axis=1, inplace=False)\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        if device is None:\n",
    "            self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        else:\n",
    "            self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.df.iloc[idx]\n",
    "        label = self.df_labels.iloc[idx]\n",
    "        tensor_data = torch.tensor(image, device=self.device, dtype=torch.float32).unsqueeze(0)\n",
    "        tensor_label = torch.tensor(label, device=self.device, dtype=torch.float32)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return tensor_data, tensor_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(device, lstm, loss_fn, optimizer, batch_size, loader, verbose=True):\n",
    "    lstm = lstm.train()\n",
    "    train_loss = 0\n",
    "    with torch.autograd.set_detect_anomaly(True):\n",
    "        for batch, (X, y) in enumerate(loader):\n",
    "            X, _ = X.to(device), y.to(device)\n",
    "        # Compute prediction error\n",
    "            pred = lstm(X)\n",
    "            loss = loss_fn(pred, X)\n",
    "\n",
    "        # Backpropagation\n",
    "\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            train_loss += loss.item()\n",
    "            if (batch % 100 == 0) and verbose:\n",
    "                loss, current = loss.item(), (batch + 1) * len(X)\n",
    "                print(f\"loss: {loss:>7f}  [{current:>5d}/{batch_size:>5d}]\")\n",
    "    avg_loss = train_loss / len(loader)\n",
    "    return avg_loss\n",
    "\n",
    "def test(device, lstm, loss_fn, batch_size, loader, verbose = True):\n",
    "    lstm = lstm.eval()\n",
    "    eval_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch, (X, y) in enumerate(loader):\n",
    "            X, _ = X.to(device), y.to(device)\n",
    "\n",
    "            pred = lstm(X)\n",
    "            loss = loss_fn(pred, X)\n",
    "\n",
    "            eval_loss += loss.item()\n",
    "            if (batch % 100 == 0) and verbose:\n",
    "                loss, current = loss.item(), (batch + 1) * len(X)\n",
    "                print(f\"loss: {loss:>7f}  [{current:>5d}/{batch_size:>5d}]\")\n",
    "    avg_loss = eval_loss / len(loader)\n",
    "    return avg_loss\n",
    "\n",
    "def validation(device, lstm, loss_fn, loader):\n",
    "    lstm = lstm.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch, (X, y) in enumerate(loader):\n",
    "            X, _ = X.to(device), y.to(device)\n",
    "            pred = lstm(X)\n",
    "            loss = loss_fn(pred, X)\n",
    "            val_loss += loss.item()\n",
    "    avg_loss = val_loss / len(loader)\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "len_none_attack_samples = len(normalized_test_df[normalized_test_df[\"attack\"]==0])\n",
    "training_data = CustomDataset(normalized_train_df[normalized_train_df[\"attack\"]==0])\n",
    "validiation_data = CustomDataset(normalized_test_df[normalized_test_df[\"attack\"]==0][:len_none_attack_samples//2])\n",
    "test_data = CustomDataset(normalized_test_df[normalized_test_df[\"attack\"]==0][len_none_attack_samples//2:])\n",
    "validation_dataloader = DataLoader(validiation_data, batch_size=batch_size, shuffle=True)\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1E-4\n",
    "lstm_small = LSTMAutoEncoder(nb_feature=41, num_layers=1, hidden_size=25, device=device)\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer_small = torch.optim.Adam(lstm_small.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10 \n",
    "train_loss = [0]*EPOCHS\n",
    "val_loss = [0]*EPOCHS\n",
    "for i in range(0, EPOCHS):\n",
    "    train_loss[i] = train(device, lstm_small, loss_fn, optimizer_small, batch_size, train_dataloader, verbose=False)\n",
    "    val_loss[i] = validation(device, lstm_small, loss_fn, validation_dataloader)\n",
    "test_loss = test(device, lstm_small, loss_fn, batch_size, test_dataloader, verbose=False)\n",
    "print(train_loss)\n",
    "print(val_loss)\n",
    "print(test_loss)\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10, 6))\n",
    "x = [i for i in range(0, len(train_loss))]\n",
    "sns.lineplot(x=x, y=train_loss, ax=ax, label=\"train loss\")\n",
    "sns.lineplot(x=x, y=val_loss, ax=ax, label=\"validation loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1E-4\n",
    "lstm_high = LSTMAutoEncoder(nb_feature=41, num_layers=1, hidden_size=100, device=device)\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer_high = torch.optim.Adam(lstm_high.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10 \n",
    "train_loss = [0]*EPOCHS\n",
    "val_loss = [0]*EPOCHS\n",
    "for i in range(0, EPOCHS):\n",
    "    train_loss[i] = train(device, lstm_high, loss_fn, optimizer_high, batch_size, train_dataloader, verbose=False)\n",
    "    val_loss[i] = validation(device, lstm_high, loss_fn, validation_dataloader)\n",
    "test_loss = test(device, lstm_high, loss_fn, batch_size, test_dataloader, verbose=False)\n",
    "print(train_loss)\n",
    "print(val_loss)\n",
    "print(test_loss)\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10, 6))\n",
    "x = [i for i in range(0, len(train_loss))]\n",
    "sns.lineplot(x=x, y=train_loss, ax=ax, label=\"train loss\")\n",
    "sns.lineplot(x=x, y=val_loss, ax=ax, label=\"validation loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDOneClassSVM\n",
    "clf = SGDOneClassSVM()\n",
    "for batch, (X, y) in enumerate(train_dataloader):\n",
    "    X, _ = X.to(device), y.to(device)\n",
    "    X = X.squeeze(1)\n",
    "    clf.partial_fit(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.array(test_df.drop(\"attack\", axis=1, inplace=False))\n",
    "Y = np.array(test_df[\"attack\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_target_map(item):\n",
    "    map = { \n",
    "        1:0,\n",
    "        -1:1\n",
    "      }\n",
    "    return map[item] \n",
    "pred_x = clf.predict(X)\n",
    "pred_map = map(outlier_target_map, pred_x)\n",
    "pred_x = np.array(list(pred_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(Y, pred_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
    "print(f\"Accuracy {accuracy_score(Y, pred_x)}\")\n",
    "print(f\"Recall {recall_score(Y, pred_x)}\")\n",
    "print(f\"Precesion {precision_score(Y, pred_x)}\")\n",
    "print(f\"F1 {f1_score(Y, pred_x)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM Auto Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_reconstruction_loss(data, model):\n",
    "    tensor_x = torch.tensor(data, dtype=torch.float32)\n",
    "    tensor_x = tensor_x.unsqueeze(1)\n",
    "    reconstructions = model(tensor_x)\n",
    "    reconstructions = reconstructions.detach().numpy()\n",
    "    reconstruction_errors = np.mean(np.abs(data - reconstructions), axis=1)\n",
    "    return np.sum(reconstruction_errors, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_outliers_reconstruction(data, model, threshold=None):\n",
    "    model.eval()\n",
    "    reconstruction_errors = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for point in data:\n",
    "            tensor_x = torch.tensor(point, dtype=torch.float32).unsqueeze(0).unsqueeze(0) \n",
    "            reconstruction = model(tensor_x)\n",
    "            reconstruction = reconstruction.squeeze().numpy() \n",
    "            error = np.mean(np.abs(point - reconstruction)) \n",
    "            reconstruction_errors.append(error)\n",
    "\n",
    "    if threshold is None:\n",
    "        # Default: use a threshold based on percentiles\n",
    "        threshold = np.percentile(reconstruction_errors, 95)\n",
    "\n",
    "    predictions = [1 if e > threshold else 0 for e in reconstruction_errors]\n",
    "\n",
    "    return predictions, reconstruction_errors, threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y, _, _ = predict_outliers_reconstruction(X, lstm_high)\n",
    "print(f\"Accuracy {accuracy_score(Y, pred_y)}\")\n",
    "print(f\"Recall {recall_score(Y, pred_y)}\")\n",
    "print(f\"Precesion {precision_score(Y, pred_y)}\")\n",
    "print(f\"F1 {f1_score(Y, pred_y)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y, _, _ = predict_outliers_reconstruction(X, lstm_small)\n",
    "print(f\"Accuracy {accuracy_score(Y, pred_y)}\")\n",
    "print(f\"Recall {recall_score(Y, pred_y)}\")\n",
    "print(f\"Precesion {precision_score(Y, pred_y)}\")\n",
    "print(f\"F1 {f1_score(Y, pred_y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combiniation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "clf = KNeighborsClassifier()\n",
    "X = np.array(test_df.drop(\"attack\", axis=1, inplace=False))\n",
    "Y = np.array(test_df[\"attack\"])\n",
    "\n",
    "tensor_x = torch.tensor(X, dtype=torch.float32)\n",
    "tensor_x = tensor_x.unsqueeze(1)\n",
    "X = lstm_small.encoder(tensor_x)\n",
    "X = X.squeeze(1)\n",
    "X = X.detach().numpy()\n",
    "clf.fit(X, Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pred_y = clf.predict(X)\n",
    "#pred_map = map(outlier_target_map, pred_y)\n",
    "#pred_y = np.array(list(pred_map))\n",
    "print(f\"Accuracy {accuracy_score(Y, pred_y)}\")\n",
    "print(f\"Recall {recall_score(Y, pred_y)}\")\n",
    "print(f\"Precesion {precision_score(Y, pred_y)}\")\n",
    "print(f\"F1 {f1_score(Y, pred_y)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab = [0, 0]\n",
    "for i in pred_y:\n",
    "    ab[i] += 1\n",
    "ab"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
